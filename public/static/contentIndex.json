{"Knowledge/Computer-Science/Courses/Computational-Neuroscience/Firing-Rate-Models":{"slug":"Knowledge/Computer-Science/Courses/Computational-Neuroscience/Firing-Rate-Models","filePath":"Knowledge/Computer Science/Courses/Computational Neuroscience/Firing-Rate Models.md","title":"Firing-Rate Models","links":[],"tags":[],"content":"Firing-rate models simplify neuron dynamics by considering firing-rate as the main representation of information processing in the brain. Firing-rate models look at units, groups of neurons whose mean activity is similar\nRecurrent feedback is input to a unit that depends on the unit’s own activity. Of course, units also impact each other. \nFiring-rate models are simpler and therefore faster. This is great because it gives them more explanatory power in describing network behavior. However, in reality changes on a small timescale do affect how a system reacts. \nMean-field theory ignores correlations between variables and smooths out the effect of fluctuations\nFiring-rate models are mean-field theories since they only represent mean firing rate.\nThese models will use different increasing functions to represent a neuron’s firing rate according to a given input. For example, sigmoid (\\frac{1}{1+e^{-x}}), power-law function, threshold linear response, etc…\nSimulation requires solving a set of coupled ODE.\n\nThe inputs to the units determine their firing rates\nThe firing rates determine the new inputs\n\n\\tau_{r_{i}}\\frac{dr_{i}}{dt}=-r_{i}+f_{i}(\\left\\{ W_{ji}s_{j} \\right\\})\n\\tau_{s_{i}}\\frac{ds_{i}}{dt}=-s_{i}+F(r_{i})\n\nW_{ji} denotes the strength of connection from unit j to unit i\ns_{i} denotes the fraction of downstream synaptic channels open\n\nHere we will simplify f_{i}(\\left\\{ W_{ji}s_{j} \\right\\})=f_{i}\\left( \\sum\\limits_{j}W_{ji}s_{j} \\right)=f_{i}(S_{i})\nS_{i} simplify represents the total synaptic input\n\n\nF(r_{i}) is the synaptic gating function, and is usually taken as a linearly increasing function of the presynaptic firing rate. If the firing rate has some maximum value, then it is easy to write F(r_{i})=\\frac{r_{i}}{r_{i}^{(\\text{max})}}\n\nIf \\tau_{s_{i}}\\ll \\tau_{r_{i}} then one can treat it as updating instantly and only solve one set of equations,\n\\tau_{r_{i}}\\frac{dr_{i}}{dt}=-r_{i}+f_{i}\\left( \\sum\\limits_{j}^{}W_{ji}\\frac{r_{j}}{r_{j}^{(\\text{max})}} \\right)\nNote that Dale’s principle is not relevant with regards to units (because why would it)\nA bistable unit can maintain spiking activity at two distinct firing rates when receiving the same level of input (typically zero).\n\nThe lower firing rate is either quiescence or a low firing rate corresponding to spontaneous neural activity.\nThe higher firing rate is initiated by an excitatory stimulus and persists afterwards\nThis kind of unit acts as a memory circuit; its activity is history-dependent. These have been recorded in monkeys during short-term memory tasks. An antagonist of NMDA prevents such responses, suggesting their important in short-term memory maintenance.\n\nBistability requires a process of positive feedback to lead away from one equilibrium and a process of negative feedback to settle at a new equilibrium\nA supralinear function’s gradient increases along the x-axis, also called a convex function\nThere are several firing rate limiting processes,\n\nFiring rate saturation; real neurons have hard physical limits on firing rates \nThe number of available receptors limits the firing rate, so neurotransmitters with slower time-constants can stabilize activity\nThe number of release-ready vesicles also limits the input to postsynaptic cells.\n\nFixed points are points of the system in which all variables stop changing. There are stable and unstable fixed points, which depends on the rate and feedback curves.\nThe synaptic time constant \\tau_{s} limits the mean synaptic response \\langle s \\rangle due to a presynaptic neuron firing spikes as a Poisson process at rate r via\n\\langle s \\rangle=\\frac{\\alpha p_{r}r\\tau_{r}}{1+\\alpha p_{r}r\\tau_{s}} where p_{r} is the release probability of each docked vesicle per spike and \\alpha is the maximum fraction of postsynaptic receptors bound by neurotransmitters when all vesicles are released\nIn a simple model s=F(r). If the dynamics of s are simulated then a more complex equation is,\n\\frac{ds}{dt}=-\\frac{s}{\\tau_{s}}+\\alpha p_{r}r(1-s)\nThe effective time constant for changes in s following changes in r becomes \\tau_{eff }(r)=\\frac{\\tau_{s}}{1+\\alpha p_{r}r\\tau_{s}}\nIn the simplest feedback curve of a bistable system, the feedback and firing rate curves intersect at three points, two of which are stable. A line attractor, on the other hand, is a continuous range of values in which a system is stable.\nThese line attractors can act as integrators and effectively sum up inputs. Any input moves the system along its line where it stays, an effect called parametric memory.\nHowever, it is not entirely clear if neural circuits ever really produce line attractors.\nDecision-Making Circuits\nModels of decision-making address three distinct issues,\n\nAccounting for experimental data, such as the proportion and time taken for different responses, depending on a stimulus\nAscertaining which models produce more optimal behavior\nAccounting for biological constraints in the makeup of neural circuits\n\nIntegration of evidence is the optimal way of accumulating information from a stimulus\nThe Wang model accounts for the activity observed with regards to decisions between two alternatives. It shows how two groups of neurons corresponding to the two alternatives could both reproduce the behavior and the ramping activity of the “winning” group of neurons\nWinner-takes-all circuits have multiple distinct units within which only one unit can sustain high activity, because of cross-inhibition\nDuring perceptual decisions, neural activity appears to ramp up gradually over a longer time period (closer to a second). This requires greater fine-tuning in a circuit comprised of neurons with time constants on the order of a few milliseconds\n\\tau \\frac{dr_{1}}{dt}=-r_{1}+W_{s}r_{1}+W_{x}r_{2}+i_{1}\n\\tau \\frac{dr_{2}}{dt}=-r_{2}+W_{s}r_{2}+W_{x}r_{1}+i_{2}\n\\tau \\frac{dr_{d}}{dt}=-r_{d}+(W_{s}-W_{x})r_{d}+i_{d}\n\\frac{dr_{d}}{dt}=-\\frac{r_{d}}{\\tau_{eff}}+\\frac{i_{d}}{\\tau}\nPerfect integration of evidence is when W_{s}=W_{x}=1 and \\tau_{eff}\\to \\infty\nForced response paradigm is a study in which the time of a response is fixed, as opposed to a free response paradigm\nThe relative cost of an incorrect response versus a slower correct response determines whether faster decisions with more errors are optimal or not. If the goal is to maximize reward rate, then longer intervals between trials favors slower, more accurate responses\nIn general, there is a speed-accuracy tradeoff, where faster responses are less accurate\nAn alternative to the gradual ramping of neural activity arising from integration is the possibility of abrupt state-transitions between different stable states. This would appear similar in measurements that average over many samples\nBias is a tendency to prefer one alternative over others before a stimulus is presented. If prior information indicates that one choice is more likely to be the correct one, then we should bias our choice according to such a prior by Bayes’ Theorem. The same effect should happen if one outcome is more rewarding.\nIn a model, one can achieve bias by applying constant input to the favored alternative. The circuit could also be initialized in a state closer to one threshold.\nExcitatory and Inhibitory Feedback\nEven on the larger scale of firing rate modeling, neural systems reveal oscillatory tendencies. Oscillatory fields are created when neural activity is synchronized among many neurons aligned in a similar direction.\nOscillations vary with our mental state and are task dependent. The gamma rhythm is in the range of 30-80 hertz and is associated with increased attention and memory processing in its localized area.\nA subset of models for gamma oscillations relies on the coupling between excitatory neurons (pyramidal cells) and inhibitory neurons (fast-spiking interneurons). These PING models rely on self-excitation and feedback inhibition\nIn the PING model, oscillations happen from a repeating cycle of quick self-excitation, followed by decaying inhibition.\nOther models use only synchronized inhibition to create oscillations. These models rely on either outside excitation or high spontaneous neural activity.\nPower spectrum is a plot of oscillatory power versus frequency to indicate the dominant frequencies in any time-dependent signal\nThe Fourier transform is a way to separate a time-dependent function into its oscillating components\nOrientation Selectivity\nThe primary visual cortex or V1 is the region at the back of the brain which receives most direct visual input from retina via the thalamus\nThe lateral geniculate nucleus is a region of the thalamus receiving inputs from the optic nerve\nOrientation selectivity is the preference of many neurons in V1 for edges or gratings oriented at a particular angle\nOne interesting feature is the way neurons respond to higher contrast. They are contrast invariant, which is to say the amplitude of their tuning curve increases but does not broaden as contrast increases. This means V1 neurons cannot only receive input from the excitatory thalamic neurons, since then you would expect the tuning curve to broaden.\nA couple of models could be used to represent this feature,\n\nExcitatory cells receiving inhibition from opposite orientations. If inhibition scales with contrast in the same way as excitation scales with contrast then the tuning curve would seem to stay the same shape\nAnother model uses recurrent feedback in the excitatory unit along with an inhibitory unit\n\nThese models imply a ring-like connectivity structure, so that neurons of perpendicular selectivity can inhibit each other.\nA ring attractor is a network in which neurons can be labelled by a variable with circular symmetry, such as orientation, and thus can be arranged into a ring\nIf the recurrent excitatory feedback is increased then activity can remain after stimulus offset, acting as a short term memory\nA circuit can have three classes of stable activity\n\nOnly inactive state, can produce contrast-invariant responses during stimulus presentation\nActive and inactive state, can provide memory for the location of a stimulus\nOnly active state, can provide information that always exists (like body position)\n\n… really need more understanding …\nSimulations suggest that angular integration is more robust in a ring model with inhibitory feedback connections than with excitatory feedback connections"},"Knowledge/Computer-Science/Courses/Computational-Neuroscience/Learning-and-Plasticity":{"slug":"Knowledge/Computer-Science/Courses/Computational-Neuroscience/Learning-and-Plasticity","filePath":"Knowledge/Computer Science/Courses/Computational Neuroscience/Learning and Plasticity.md","title":"Learning and Plasticity","links":[],"tags":[],"content":"Hebbian plasticity\nHebbian plasticity is a change in synaptic strength produced by a positive correlation between presynaptic and postsynaptic spikes \nThe mechanisms generally depend on an increase in calcium concentration within the postsynaptic cell in the vicinity of the synapse, either through calcium channels, NMDA receptors, or internal stores\nA presynaptic spikes causes glutamate to be released onto the postsynaptic receptors for a short amount of time; then, if the postsynaptic cell’s membrane potential rises, the NMDA channels (glutamate receptors) open and admit calcium ions\nLong-term depression is necessary to counteract the long-term potentiation that results from Hebbian plasticity. Therefore, active cells ought to weaken their excitatory connections to less active excitatory cells. Even with this change, the system would seem to tend towards an extreme where feedback causes a subset of synapses to be strengthened to their biological maximum\nHebbian plasticity can be simulated in a simple way with firing rate models, where the connection strength between two units changes according to the covariation in their firing rates\n\\tau\\frac{ dW_{ij}}{dt}=\\Theta(r_{i}-r_{r})\\cdot\\Theta(r_{j}-r_{r}) where \\Theta(x) is the Heaviside function (1 if x is positive, 0 otherwise) and r_{r} is the threshold rate for inducing plasticity\nOne could also use a continuous rate of change function, like \\tau \\frac{dW_{ij}}{dt}=r_{i}r_{j}(r_{j}-r_{r}) (positive is they are the same sign, negative otherwise)\nThese networks can demonstrate,\n\nPattern completion, producing a correct pattern based on prior examples when provided with a partial or corrupted input\nPattern separation, producing proper responses for different patterns, depending on which pattern the input more closely resembles\n\nSpike-Timing Dependent Plasticity\nSTDP means changes in connection strength that depend on the relative timing of spikes, not only rates\nThe rules listed so far reflect the shortening version of Hebb’s postulate, “cells that fire together wire together.” In reality, causality and timing are important\nIn some experiments, strengthening of the synaptic connection in one direction corresponded with weakening in the other direction\n\nIn addition, a minimum number rate of pairs is required\nA minimum and maximum number of pairings will produce significant change\nA high amount of variability in synaptic change is observed\n\nWhile these factors will be ignored for simplification, a few other factors are necessary to consider\n\nWhether to only consider successive pairs of spikes are all potential pairs with units (matters depending on spike rate)\nWhether to use a batch updating method or a continuous updating method\n\nA standard model of STDP looks like,\n\\Delta W_{ij}=-A_{-}\\cdot e^{-(t_{i}-t_{j})/\\tau_{-}} with t_{i}&gt;t_{j}\n\\Delta W_{ij}=-A_{+}\\cdot e^{-(t_{j}-t_{i})/\\tau_{+}} with t_{j}&gt;t_{i}\nThis can be updated at each spike or after a batch, but for the continuous method it is good to continually update the variables,\n\\frac{da_{i}}{dt}=-\\frac{a_{i}}{\\tau_{+}} (total presynaptic impact)\n\\frac{de_{i}}{dt}=-\\frac{e_{i}}{\\tau_{-}} (total postsynaptic impact)\nThen at each postsynaptic spike j, the strength is incremented by \\Delta W_{ij}=A_{+}a_{i} and at each presynaptic spike i, the strength is decremented by \\Delta W_{ij}=A_{-}e_{j}\nThe area between each curve and the x-axis is given by A_{+}\\tau_{+} and A_{-}\\tau_{-}. If spikes are completely uncorrelated then potentiation is proportional to the former and depression is proportional to the latter. In this scenario, it’s necessary that A_{-}\\tau_{-}&gt;A_{+}\\tau_{+}, so that\n\nIncreasing a subset of synapses strengthens their postsynaptic activity\nIncreased postsynaptic activity leads to more pairs of spikes and shorter time intervals, leading to enhanced plasticity\nIf other presynaptic neurons have uncorrelated spikes then their input synapses become depressed\nOverall, this creates competition, where increase in one subset decreases another\n\nSTDP can enable sequence learning, where neurons learn to fire in a particular order\nTo better model STDP interactions, more complicated protocols exist, like triplet STDP\nNote that empirical rules for altering synaptic strength are just approximations, ignoring many intracellular processes. For many neurons, coming up with empirical rules is more efficient and simpler\n\nFor example, sometimes calcium spikes are necessary for inducing plasticity\nSince calcium spikes are correlated with high postsynaptic activity, a rule can be made simply requiring a high rate of postsynaptic spikes for plasticity\n\nNMDA receptors on calcium channels are an important mechanism in enabling STDP. NMDA receptors on a postsynaptic cell require the binding of glutamate (released by presynaptic cells) and depolarization (caused by postsynaptic spiking). The time constant of depolarization is much shorter than that of NMDA receptors, so causality is enforced\nMembrane potential is usually also an important factor in models of synaptic plasticity\n\nDepression arises when a presynaptic spike follows a period of depolarization\nPotentiation arises at a high postsynaptic membrane potential if it follows a presynaptic spike\n\nIf another variable would be simulated, then calcium is the best choice. Since its concentration typically varies a lot, each synapse requires a separate variable. Somatic calcium concentration can be used for cell-wide homeostatic regulation\nHomeostasis\nNegative feedback is very important in maintaining a system. Neural circuits have their own needs for regulation, such as the right level of firing\nNeural circuit behavior depends sensitively on parameters like synaptic connection or different conductances. These variables are also maintained over time through the production and processing of proteins\nNeurons must also maintain the right level of sensitivity to inputs. One way to achieve this is if a neuron has a mechanism to monitor its firing rate and adjust the strengths of synapses or excitability accordingly\nThis has been observed to happen; if neurons are prevented from spiking with tetrodotoxin for a day or more, they become more excitable, whereas applications of bicuculline lead to a reduction in excitatory synaptic strengths\nA calcium-dependent negative feedback pathway could support homeostasis; Increasing firing rate leads to an increase in somatic calcium, reducing the conductance of sodium channels and decreasing the neuron’s firing rate\nThese mechanisms can be simulated in a simple way by decreasing excitability according to average firing rate,\n\\tau_{g}\\frac{dG_{E}}{dt}=r_{goal}-r(t) or \\tau_{th}\\frac{dV_{th}}{dt}=r_{goal}-r(t) with extremely long time scales\nHomeostatic feedback is likely necessary to ensure neurons retain their role as they change in size, since the rate of protein transcription would seem to change at a rate non-proportional to surface area\nSupervised Learning\nSupervised learning is learning that depends on feedback based on the outcome of a behavior\nWhile unsupervised learning allows our brains to extract common features from the environment\nReinforcement learning is supervised learning where the feedback signal is a scalar quantity; A better than expected outcome produces a positive reinforcement signal while a worse outcome produces a negative signal\nReinforcement learning feedback has no direction, it only rewards or punishes so is easiest to look at in situations with discrete alternatives\nNeuroeconomics is the study of the neural basis of decision making in situations where the outcomes of those decisions are quantifiable\nUnconditioned stimulus cause a response without any need for training\nConditioned stimulus cause a response after training\nClassical conditioning is associating a conditioned stimulus with an unconditioned stimulus\nOperant conditioning (also called instrumental conditioning) is the altering of an animal’s behavior such that some responses are rewarded and some are punished. This is more relevant to neural circuits\nDopamine plays a key role in conditioning and generally signals a reward prediction error\nReinforcement learning can help an animal transition from exploring possibilities to exploiting the best actions\nIn the weather prediction task, a subject must learn how well different stimuli predict a response. A three-component plasticity rule is sufficient to produce the correct behavior.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPresynaptic RatePostsynaptic RatePrediction ErrorChange in StrengthHighHigh++HighLow+-HighHigh--HighLow-+\nThis makes sense, however optimal decision-making requires accounting for the probability of the response given the stimulus, and this does not do so. For a rarer stimulus, this makes a difference\nEyeblink conditioning is an interesting experiment where an animal is conditioned to blink after hearing a tone in order to avoid a gust of air\nThe protocol could be one of delay conditioning if the tone remains on or trace conditioning in which the tone leaves a pause (this difference is important because trace conditioning requires the hippocampus for short-term memory)"},"Knowledge/Computer-Science/Courses/Computational-Neuroscience/Preliminary-Information":{"slug":"Knowledge/Computer-Science/Courses/Computational-Neuroscience/Preliminary-Information","filePath":"Knowledge/Computer Science/Courses/Computational Neuroscience/Preliminary Information.md","title":"Preliminary Information","links":[],"tags":[],"content":"General Background\nNearly all of single neuron modeling revolves around calculating causes and effects of the action potential\nDefinition: The membrane potential V_{m} is the highly variable potential difference across the cell’s membrane\nThe key to modeling is ordinary differential equations which are solved computationally\nDefinition: A variable is a property of the system that changes while a parameter is a fixed property. Differential equations describe the rate of change of a variable in terms of all quantities.\nThe simplest and most common differential equation in this course takes the form of \\frac{dV_{m}}{dt}=G_{L}(E_{L}-V_{m}) /C_{m}, where\n\nE_{L} is the leak potential, the value that V_{m} returns to after any temporary charge imbalance leaks away\nG_{L} is the leak conductance, the ability of charge to leak in and out of the cell through its membrane, which increases with the surface area of the cell\nC_{m} is the membrane capacitance, the capacity of the cell’s membrane to store electrical charge, which increases with the surface area of the cell\n\nCellular Background\nFor the most part, neurons can function as normal cells in the body. Transmembrane proteins can act as ion channels which contribute to neural function. Additional terms are relevant to neurons:\n\nThe soma is the main body of a cell, containing the nucleus\nThe process is elongated and extends far from the soma\nThe dendrites are branched processes that transfer input from nearby cells/receptors to the soma\nThe axons are especially thin branched processes that convey neural activity generated in the soma. They transmit action potentials, spikes in the electrical potential difference across the cell membrane.\nVesicles are small containers of biochemical molecules, which carry neurotransmitters. These bind to neighboring neuron receptors which continues the process across multiple cells.\nGlial cells support neural activity and are less studied\n\nOn a circuit level, neurons are viewed in terms of their ramification, the amount of branching of their dendrites and axon\nThe complete circuit diagram of a particular region of a central nervous system, an animal’s areas of significant neural processing, is called its connectome\nIn some small species like C. elegans and Drosophila melanogaster, the connectome can be fully mapped out. In mammals, the connectome is more of a statistical process, coming up with a stereotypical circuit. For humans, due to scale and variability, this seems entirely unfeasible.\nOn a regional level, different parts of a brain will have distinct functions, shared across all members of a species\nThe cerebral cortex is the outer region of mammalian brains that is highly folded (with ridges/gyri and crevasses/sulci) in humans\nA topographic map is an area of the brain where neurons are positioned in an organized arrangement directly related to the arrangement of their respective stimuli\nPhysics Background\nBroadly speaking, electrical circuits can be produced whenever an electrical charge can move around. In solutions, the charge carriers are ions.\nMany chemicals are soluble and can dissociate into positively and negatively charged ions. The motion of charges is mediated by an electric field, the gradient of electric potential. Positive charges are drawn to negative potential and vice versa. Each charge produces its own electric field.\nAn electrical current is a flow of electrical charge. In our case, I_{m}=\\frac{dQ}{dt} where I_{m} is the inward membrane current and Q is the charge on the inside of the cell’s membrane.\nVoltage or potential difference is a force that moves charges. Significant potential differences arise between the inner and outer surfaces of neurons.\nCapacitance (capacity to store charge) is Q=CV where Q is charge on a surface and V is potential difference produced across that surface\nConductance (ability to let current flow) G represents how current relates to potential difference. Its inverse is resistance: V=IR and I=GV\nNeurons can control voltage through ion pumps and ion channels. Notably, cytoplasm is not a particularly good conductor; different parts of a neuron will have different voltages. Models that treat a neuron as a single point lose some of this detail.\nA time constant is defined by the amount of time needed for a system to get closer to equilibrium by a factor of 1/e. This is related to the amount of charge that is to flow and the rate of flow, which is given by the capacitance divided by the conductance.\nMathematics Background\nDifferential equations represent the way things change over time. This course will deal with ordinary differential equations which only describe individual properties of a system. These can be difficult to solve mathematically but we will just use computers to solve them numerically.\nExample: Say the velocity of a runner is modeled with v(t)=v_{max}(1-e^{-t/ \\tau})\nFor a simple equation like this we can use calculus knowledge to solve x(t)=v_{max}t-v_{max}\\tau(1-e^{-t/ \\tau})\nExample: Sometimes a more complicated dependence exists such that velocity is a function of time and position, \\frac{dx}{dt}=v(x,t)=v_{0}(1+x(t)^{2})\nOutside knowledge tells us that a solution is x(t)=\\tan(v_{0}t)\nDefinition: Exponential decay occurs in a system with an equilibrium point and a rate of change proportional to distance to equilibrium\nExample: Say we want to model heat entering a house, K\\frac{dT}{dt}=-A+B(T_{ext}-T) where K is the heap capacity of the house, A is the rate of heat extraction, T_{ext} is the outside air temperature, and B is a measure of how easily heat flows through the walls\nThis is both an ODE and linear, since dT is proportional to T. The function is therefore called a linear first order ordinary differential equation.\nT_{eq} is when \\frac{dT}{dt}=0\nT_{eq}=T_{ext}-A/B\nLinear first order ODE can also be written as \\frac{dT}{dt}=\\frac{T_{eq}-T}{\\tau} where \\tau is the time constant of the system, the inverse of the gradient. In this form it is clear that this system is stable and will tend towards equilibrium.\nIf the initial temperature is T_{0} at t=0 then the general equation is T(t)=T_{eq}+(T_{0}-T_{eq})e^{-t /\\tau}\nIn our example T(t)=T_{ext}-A / B(1-e^{-Bt})\nProof:\n\\frac{dx}{dt}=\\frac{x_{eq}-x}{\\tau}\n\\frac{dx}{x_{eq}-x}=\\frac{dt}{\\tau}\n[-\\ln(x_{eq}-x)]^{x(t)}_{x(0)}\n\\ln\\left( \\frac{x_{eq}-x(t)}{x_{eq}-x(0)} \\right)=-t / \\tau\n\\frac{x_{eq}-x(t)}{x_{eq}-x(0)}=e^{-t / \\tau}\nx(t)=x_{eq}+(x(0)-x_{eq})e^{-t/ \\tau}\nExample: One model in neuroscience uses gating variables \\alpha and \\beta to represent the fraction of closed ion channels opening and open ion channels closing\nSay N_{O} is the number of open channels, N_{C} is the number of closed channels, and N_{T}=N_{C}+N_{O} is the total. Then the rate of closed channels opening is \\alpha N_{C} and the rate of open channels closing is \\beta N_{O}. Then the rate of change of the number is the opening ones minus the closing ones: \\frac{dN_{O}}{dt}=\\alpha(N_{T}-N_{O})-\\beta N_{O}.\nThe gating variable s is N_{O} / N_{T}, so \\frac{ds}{dt}=\\alpha(1-s)-\\beta s\nNow \\frac{ds}{dt}=\\frac{s_{eq}-s}{\\tau_{s}} where s_{eq}=\\frac{\\alpha}{\\alpha+\\beta} and \\tau_{s}=\\frac{1}{\\alpha+\\beta}\nTherefore, s(t)=s_{eq}+(s(0)-s_{eq})e^{-t / \\tau_{s}}\nThis is a good model for two-state systems. This can also be applied to multiple two-state variables as long as they are independent. As in, if multiple independent states contribute to whether a gate is open or not, then we can multiply the gating variables together to evaluate the total probability.\nLinear algebra is important for large amounts of data. I just learned it so no need to recap.\nProbability! The probability of an event’s occurrence ranges from 0 to 1:\nP(A\\cup B)=P(A)+P(B) if A and B are mutually exclusive\nP(A\\cup B)=P(A)+P(B)-P(A\\cap B) if A and B are not mutually exclusive\nP(A\\cap B)=P(A)P(B) if A and B are independent\nA and B are independent if P(A|B)=P(A) where P(A|B) means “the probability of A given B”\nA and B are positively correlated if P(A|B)&gt;P(A)\\iff P(B|A)&gt;P(B)\\iff P(A\\cap B)&gt;P(A)P(B)\nP(A\\cap B)=P(A)P(B|A) even if A and B are not independent\nBy symmetry, we find Bayes’ Theorem, P(B|A)=\\frac{P(B)P(A|B)}{P(A)}\nP(A) and P(B) are called priors\nExample: Suppose we have five coins, four are fair and one produces heads with probability 3 /4. You choose one of the coins at random and get 3 heads. What is the probability it is the biased coin?\nP(B|A)=\\frac{(1 / 5)(3 / 4)^3}{(1 / 5)(3 / 4)^3+(4 / 5)(1 / 2)^{3}}=27 / 59\nPowerful conceptual idea!\nIn this case, the prior probability of the biased coin was 1 / 5. However after flipping 3 times, the posterior probability is 27 / 59\\sim 0.458. This posterior probability could be used as a prior for another experiment.\nMATLAB\nThis stuff is not so difficult. Pretty straightforward. But the section includes some stuff on solving ODEs.\nEssentially, given an ODE \\frac{dx}{dt}=f(x,t), one can easily write \\frac{x_{n+1}-x_{n}}{\\Delta t}=f(x_{n},t_{n})\nSo x_{n+1}=x_{n}+f(x_{n},t_{n})\\Delta t\nThe error with this method is accurate to the order of \\Delta t/T\nOther methods, like backward Euler and Runge-Kutta methods are more accurate.\nOne advantage of forward Euler is how it is relatively simple to add white-noise, i.e. random variation to the system,\n\\frac{dx}{dt}=f(x,t)+\\sigma\\cdot w(t) where \\sigma is a scaling factor for the noise\nx_{n+1}=x_{n}+f(x_{n})\\Delta t+\\sigma \\overset{\\sim}{w}_{n}\\sqrt{ \\Delta t }, where \\overset{\\sim}{w}_{n} is a random number selected from a distribution with zero mean and unit variance\nGaussian is a common choice P(\\overset{\\sim}{w}_{n})=\\frac{1}{\\sqrt{ 2\\pi }}e^{-\\overset{\\sim}{w}_{n}^{2} / 2} and can be obtained in MATLAB easily via randn()"},"Knowledge/Mathematics/Courses/Probability/Expectation":{"slug":"Knowledge/Mathematics/Courses/Probability/Expectation","filePath":"Knowledge/Mathematics/Courses/Probability/Expectation.md","title":"Expectation","links":["Knowledge/Mathematics/Courses/Probability/Analyzing-Quick-Sort"],"tags":[],"content":"Recall, the expected value of a random variable X is E[X]=\\sum_{x}^{}xp(x) if X is discrete and E[X]=\\int_{-\\infty}^{\\infty}xf(x) \\, dx if X is continuous\nIf P\\{ a\\leq X\\leq b \\}=1 for some a and b then a\\leq E[X]\\leq b\nIf X and Y have a joint probability mass function p(x,y) then E[g(X,Y)]=\\sum_{y}^{}\\sum_{x}^{}g(x,y)p(x,y)\nThe continuous analog to this is E[g(X,Y)]=\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}g(x,y)f(x,y) \\, dx \\, dy\nWe can apply this to find E[X+Y]=\\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} (x+y)f(x,y)\\, dx\\, dy=\\int_{-\\infty}^{\\infty} xf_{X}(x)\\, dx+\\int_{-\\infty}^{\\infty} yf_{Y}(y)\\, dy\n=E[X]+E[Y]\nAn inductive argument shows E[X_{1}+\\dots+X_{n}]=E[X_{1}]+\\dots+E[X_{n}]. This property is extremely useful, and is often referred to as linearity of expectation. \nWe can break down a binomial random variable X into Bernoulli random variables to show that E[X]=np\nWe can break down a negative binomial random variable X into geometric random variables with parameter p (and expectation E[X_{i}]=1 /p) to show that E[X]=\\frac{r}{p}\nWe can also break down a hypergeometric random variable into indicator variables Y_{i}=1 if the ith ball selected is white and 0 otherwise. Any ball is equally likely to be the ith ball, so E[Y_{i}]=\\frac{m}{N} and E[X]=\\frac{nm}{N}\nExample: Analyzing Quick-Sort\nLet f be a function on a finite set A and suppose we want to find m=\\max_{s \\in A}f(s). We can find a lower-bound probabilistically, since m\\geq E[f(S)]\nThe textbook uses this to find bounds on Hamiltonian paths in an example\nObserve: If X is the number of events that occur from A_{1},\\dots,A_{n}, then \\binom{X}{2} is the number of pairs of events that occur. We can see that \\binom{X}{2}=\\sum\\limits_{i&lt;j}^{}I_{i}I_{j}\nWe get E[X^{2}]-E[X]=2\\sum\\limits_{i&lt;j}^{}P(A_{i}A_{j})\nAn extension of this is E[\\binom{X}{k}]=\\sum\\limits_{i_{1}&lt;i_{2}&lt;\\dots&lt;i_{k}}^{}P(A_{i_{1}}A_{i_{2}}\\dots A_{i_{k}})\nThese can help let us calculate \\text{Var}(X) and E[X^k]\nWe can also write E[g(X)h(Y)]=E[g(X)]E[h(Y)]\nDefinition: The covariance between X and Y is defined as \\text{Cov}(X,Y)=E[(X-E[X])(Y-E[Y])]. Expanding this yields \\text{Cov}(X,Y)=E[XY]-E[X]E[Y].\nIf X and Y are independent, then \\text{Cov}(X,Y)=0, however the converse is not true\nWe have some simple properties,\n\n\\text{Cov}(X,Y)=\\text{Cov}(Y,X)\n\\text{Cov}(X,X)=\\text{Var}(X)\n\\text{Cov}(aX,Y)=a\\text{Cov}(X,Y)\n\\text{Cov}(\\sum_{i=1}^{n}X_{i},\\sum_{j=1}^{m}Y_{j})=\\sum_{i=1}^{n}\\sum_{j=1}^{m}\\text{Cov}(X_{i},Y_{j})\n\nWith 4., we can derive \\text{Var}\\left( \\sum_{i=1}^{n}X_{i} \\right)=\\sum_{i=1}^{n}\\text{Var}(X_{i})+2 \\sum_{i&lt;j}\\text{Cov}(X_{i},X_{j})\nIf X_{1},\\dots,X_{n} are pairwise independent then \\text{Var}(\\sum_{i=1}^{n}X_{i})=\\sum_{i=1}^{n}\\text{Var}(X_{i})\nThe correlation of two random variables X and Y, denoted by \\rho(X,Y) is defined as \\rho(X,Y)=\\frac{\\text{Cov}(X,Y)}{\\sqrt{ \\text{Var}(X)\\text{Var}(Y) }}.\nWe can prove that -1\\leq\\rho(X,Y)\\leq 1 pretty simply. Say X and Y have variances given by \\sigma_{x}^{2} and \\sigma_{y}^{2},\n0\\leq \\text{Var}\\left( \\frac{X}{\\sigma_{x}}+\\frac{Y}{\\sigma_{y}} \\right)\n=\\frac{\\text{Var}(X)}{\\sigma^{2}_{x}}+\\frac{\\text{Var}(Y)}{\\sigma^{2}_{y}}+\\frac{2\\text{Cov}(X,Y)}{\\sigma_{x}\\sigma_{y}}\n=2[1+\\rho(X,Y)]\\implies-1\\leq\\rho(X,Y)\n\\rho behaves similarly to covariance but is nice and bounded. It can also be viewed as a measure of the degree of linearity between X and Y, since \\rho(X,Y)=-1 implies \\text{Var}\\left( \\frac{X}{\\sigma_{x}}+\\frac{Y}{\\sigma_{y}} \\right)=0, which means \\frac{X}{\\sigma_{x}}+\\frac{Y}{\\sigma_{y}} is constant.\nConditional Expectation\nRemember that we have p_{X|Y}(x|y)=\\frac{p(x,y)}{p_{Y}(y)}\nIt makes sense to also define E[X|Y=y]=\\sum\\limits_{x}xp_{X|Y}(x|y)\nWe denote E[X|Y] as a function of Y (whose value at y is E[X|Y=y]). This means E[X|Y] is also a random variable.\nTheorem: E[X]=E[E[X|Y]]\nThis implies E[X]=\\sum\\limits_{y}E[X|Y=y]P\\{ Y=y \\} (or E[X]=\\int_{-\\infty}^{\\infty} E[X|Y=y]f_{Y}(y)\\, dy for a continuous variable), which is just the law of total probability\nWe can also interpret this theorem as taking the weighted average of the conditional expected value of X for each Y=y\nThis theorem can help us calculate probabilities by conditioning on indicator variables\nConditional variance is also well-defined, as \\text{Var}(X|Y)=E[(X-E[X|Y])^{2}|Y]\nTo go with it, there’s a useful identity \\text{Var}(X)=E[\\text{Var}(X|Y)]+\\text{Var}(E[X|Y])\nNote: If we want to predict Y given X with a function g, our best predictor is E[Y|X]\nDefinition: The moment generating function M(t) of X is defined for all t as M(t)=E[e^{tX}]. It’s called the moment generating function because all moments can be obtained by differentiating M(T) and evaluating at t=0.\nThis assumes that \\frac{d}{dt}E[e^{tX}]=E\\left[ \\frac{d}{dt}e^{tX} \\right], which is generally true\nThe moment generating function of joint variables X_{1},\\dots,X_{n} is a multivariable function M(t_{1},\\dots,t_{n})=E[e^{t_{1}X_{1}+\\dots+t_{n}X_{n}}]. It can be proven that this function uniquely determines the join distribution of X_{1},\\dots, X_{n}. This function can also be used to find the individual moment generating functions of X_{i}."},"Knowledge/Mathematics/Courses/Probability/Overview":{"slug":"Knowledge/Mathematics/Courses/Probability/Overview","filePath":"Knowledge/Mathematics/Courses/Probability/Overview.md","title":"Overview","links":[],"tags":[],"content":"Probability is cool! Cause humans are bad at it without math\nFor this class, we have to do the reading ahead of time, since Keith does not lecture. There are lots of cool lectures online if you need help on the more basic stuff, but Keith likes the cool stuff.\nLearning goals,\n\nLearn the axioms\nLearn to compute simple probabilities/distributions\nRandom variables\nBayes’ Theorem\nConditional probabilities and expectations\nSignificance of independent random variables\nWeak Law of Large Numbers, Strong Law of Large Numbers, the Central Limit Theorem\n\nWhat the fuck is probability?!\nIdeas,\n\nMultiverse\nLimit as trials goes to infinity\nWeather reports????\n\nPhilosophically, we call #2 frequentism, since it refers to the relative frequency of an event\n(FYI, Keith recommends MATH 36B, which requires more thought towards this question.)\nWhen someone says, “the chance I get a raise is 60%,” how do we interpret this through the definition of frequency? An alternative philosophy is that the statement reflects the speaker’s beliefs, their level of confidence.\nIn statistics, confidence is a key concept. In this class, the difference is not as important, since the pure math does not care much.\nA probability space assigns probabilities to outcomes of an experiment. It is defined with the items (\\Omega,P). For any A\\subseteq\\Omega, our probability space defines P(A), where 0\\leq P(A)\\leq 1.\nSo say we roll a d6, \\Omega=\\left\\{ 1,2,3,4,5,6 \\right\\}\nLet’s define two events, A=\\left\\{ 3 \\right\\} and B=\\left\\{ 1,3,5 \\right\\}\nWe know very little about this situation (if we don’t assume the dice is fair), however we can say with certainty that P(A)\\leq P(B).\nThis is an example of monotinicity; as outcomes are added to an event, the probability increases or stays the same. When A\\subseteq B, P(A)\\leq P(B).\nProbability spaces also showcase disjoint additivity, where the probability of a sum of a collection of disjoint sets is equal to the sum of their individual probabilities. FYI, this is only valid on countable sets.\nAlso, P(\\emptyset)=0 and P(\\Omega)=1"},"Knowledge/Mathematics/Courses/Probability/index":{"slug":"Knowledge/Mathematics/Courses/Probability/index","filePath":"Knowledge/Mathematics/Courses/Probability/index.md","title":"Probability","links":[],"tags":[],"content":""},"Knowledge/Mathematics/Courses/Statistics/Hypothesis-Testing":{"slug":"Knowledge/Mathematics/Courses/Statistics/Hypothesis-Testing","filePath":"Knowledge/Mathematics/Courses/Statistics/Hypothesis Testing.md","title":"Hypothesis Testing","links":[],"tags":[],"content":"Hypothesis testing aims to choose between a null hypothesis H_{0} and an alternative hypothesis H_{1}\nA function of the observed data who dictates the outcome of our hypothesis is called a test statistic. The set of values that result in our null hypothesis’ rejection is the critical region, denoted C. The points separating C from the acceptance region is the critical value.\nThe probability a test statistic rejects H_{0} when H_{0} is true is the level of signifiance, denoted \\alpha\n\\alpha=0.01,0.05,0.1 are common\nTheorem: Let y_{1},y_{2},\\dots ,y_{n} be a random sample from a normal distribution with \\sigma. H_{0}:=\\mu=\\mu_{0}. Let z=\\frac{\\bar{y}-\\mu_{0}}{\\sigma /\\sqrt{ n }}\n\nAccept H_{1}:=\\mu&gt;\\mu_{0} if z\\geq z_{\\alpha}\nAccept H_{1}:=\\mu&lt;\\mu_{0} if z\\leq-z_{\\alpha}\nAccept H_{1}:\\mu \\neq \\mu_{0} if z\\leq-z_{\\alpha /2} or z\\geq z_{\\alpha /2}\n\nAn alternative way of formulating the same idea, the P-value of a test statistic is the probability of getting a value at least as extreme as a given statistic, assuming H_{0} is true\nWe can run binomial hypothesis tests much like the test above. When we have a “large” number of samples, we can approximate a group of samples as a normal distribution.\nWe use the condition 0&lt;np_{0}-3\\sqrt{ np_{0}(1-p_{0}) }&lt;np_{0}+3\\sqrt{ np_{0}(1-p_{0}) }&lt;n to identify large samples. This is true when the range of 3 standard deviations fall into the valid values of X.\n\\sigma=\\sqrt{ np_{0}(1-p_{0}) }\nTheorem: Let k=k_{1}+k_{2}+\\dots+k_{n} be a random sample of n Bernoulli random variables, for which 0&lt;np_{0}-3\\sqrt{ np_{0}(1-p_{0}) }&lt;np_{0}+3\\sqrt{ np_{0}(1-p_{0}) }&lt;n. We use z=\\frac{k-np_{0}}{\\sqrt{ np_{0}(1-p_{0}) }} and test as above.\nIf our inequality doesn’t hold, we use the exact binomial distribution\nP(X\\geq k)=\\sum_{x=k}^{\\infty}\\binom{n}{x}p_{0}^x(1-p_{0})^{n-x}\nP(X\\leq k)=\\sum_{x=0}^{k}\\binom{n}{x}p_{0}^x(1-p_{0})^{n-x}\n\nAccept H_{1}:=p&gt;p_{0} if P(X\\geq k)\\leq\\alpha\nAccept H_{1}:=p&lt;p_{0} if P(X\\leq k)\\leq\\alpha\nAccept H_{1}:=p\\neq p_{0} if P(X\\leq k)+P(X\\geq k)\\leq\\alpha\n\nThis is equivalent to our previous theorems, but stated terms of the CDF and \\alpha instead of the critical value and z_{\\alpha} (which is the inverse-CDF of normal)\nType I and II Errors\nWe already defined \\alpha= the probability of incorrectly rejecting H_{0}. Call this the Type I error.\n\\beta= the probability of incorrectly accepting H_{0}, is called the Type II error.\n\\beta is a function of the presumed value \\mu. If \\mu is really close to \\mu_{0} then \\beta will be high.\nDefinition: 1-\\beta is the power of a decision test, as a function of the parameter being tested. A power curve graphs this relation.\nThe power of a test diminishes as \\mu\\to \\mu_{0}. At \\mu=\\mu_{0}, 1-\\beta=\\alpha.\nA steeper power curve is a stronger test\nThe power of the Z test is a function of \\alpha, \\sigma, and n. We can improve our power by either decreasing \\alpha, decreasing \\sigma, or increasing n.\nExample: Say we’d like to test H_{0}:\\mu=100 versus H_{1}:\\mu&gt;100 with \\alpha=0.05 and 1-\\beta=0.60 when \\mu=103. What is the smallest sample size that achieves this objective? Assume a normal distribution with \\sigma=14\nWe solve this by writing a system of equations with the critical value \\bar{y}^*\n\\bar{y}^*=100+z_{\\alpha }\\cdot \\frac{14}{\\sqrt{ n }}\n\\bar{y}^*=103+z_{\\beta}\\cdot \\frac{14}{\\sqrt{ n }}\nNonnormal Decision Rules\nDecision rules on general pdfs f_{Y}(y;\\theta) work much the same\nTo test H_{0}:\\theta=\\theta_{o}, we define a decision rule in terms of a sufficient statistic \\hat{\\theta}. The critical region is the set of values of \\hat{\\theta} least compatible with \\theta_{o} and admissible under H_{1} whose total probability when H_{0} is true is \\alpha\nExample: f_{Y}(y;\\theta)= 1 /\\theta for 0\\leq y\\leq\\theta\nn=8\nH_{0}:\\theta=2.0\nH_{1}:\\theta&lt;2.0\n\\alpha=0.10\nSuppose we base the decision rule on Y&#039;_{8}, the largest order statistic. What’s \\beta when \\theta=1.7?\nSome thought about the uniform distribution makes it clear we are looking for some value P(Y&#039;_{8}\\leq c\\mid H_{0}\\text{ is true})=0.10\nf_{Y&#039;_{8}}(y;\\theta=2)=8\\left( \\frac{y}{2} \\right)^7\\cdot \\frac{1}{2} for 0\\leq y\\leq 2\n\\int_{0}^{c} 8\\left( \\frac{y}{2} \\right)^7\\cdot \\frac{1}{2}\\, dy=0.10\nc=1.50\n\\beta=P(Y&#039;_{8}&gt;1.50\\mid\\theta=1.7)=\\int_{1.50}^{1.7} 8\\left( \\frac{y}{1.7} \\right)^7\\cdot \\frac{1}{1.7}\\, dy=0.63\nThe Generalized Likelihood Ratio\nWhat is the best decision rule for choosing between H_{0} and H_{1}, and how do we show it is optimal?\nDefine \\omega as the set of unknown parameter values admissible under H_{0}\nDefine \\Omega as the set of all possible values of all unknown parameters\nDefinition: Let y_{1},y_{2},\\dots ,y_{n} be a random sample from f_{Y}(y;\\theta_{1},\\dots,\\theta_{k}). The generalized likelihood ratio is \\lambda=\\frac{\\max_{\\omega}L(\\theta_{1},\\dots,\\theta_{k})}{\\max_{\\Omega}L(\\theta_{1},\\dots,\\theta_{k})}=\\frac{L(\\omega_{e})}{L(\\Omega_{e})}\nMaximizing L(\\theta) under \\Omega (no restrictions), is accomplished by substituting \\theta_{e} into L(\\theta)\nValues closer to 1 show that the data is more compatible with H_{0}\nDefinition: A generalized likelihood ratio test (GLRT) rejections H_{0} whenever 0&lt;\\lambda\\leq\\lambda^* where P(0&lt;\\Lambda\\leq\\lambda^*\\mid H_{0}\\text{ is true})=\\alpha (\\Lambda is \\lambda expressed as a random variable)\nExpressed similarly, \\alpha=\\int_{0}^{\\lambda^*} f_{\\Lambda}(\\lambda \\mid H_{0})\\, d\\lambda\nIn many situations, f_{\\Lambda}(\\lambda \\mid H_{0}) is not known, so we must show \\Lambda is a monotonic function of a quantity W, where the distribution of W is known.\nExample:\nSo say we work with a uniform distribution, H_{0}:\\theta=\\theta_{o} and H_{1}:\\theta&lt;\\theta_{o}. \\omega=\\{ \\theta_{o} \\} and \\Omega=\\{ \\theta:0&lt;\\theta\\leq\\theta_{o} \\}\nWe have \\lambda=\\frac{(1/\\theta_{0})^n}{(1 /y_{\\text{max}})^n}=\\left( \\frac{y_{\\text{max}}}{\\theta_{0}} \\right)^n\n\\alpha=P\\left[ \\left( \\frac{Y_{\\text{max}}}{\\theta_{0}} \\right)^n\\leq\\lambda^*\\mid H_{0}\\text{ is true} \\right]\nInstead of trying to solve for \\lambda^* directly, we take W=Y_{\\text{max} } /\\theta_{0} and w^*=\\sqrt[n]{ \\lambda^* }\nf_{W}(w;\\theta_{0})=\\theta_{0}f_{Y_{\\text{max}}}(\\theta_{0}w;\\theta_{0})=\\frac{\\theta_{0}n(\\theta_{0}w)^{n-1}}{\\theta_{0}^n}=nw^{n-1} for 0\\leq w\\leq 1\nP(W\\leq w^*)\\mid H_{0}\\text{ is true}=\\int_{0}^{w^*}nw^{n_{0}1} \\, dw=(w^*)^{n}=\\alpha\nWe conclude that the GLRT calls to reject H_{0} if \\frac{y_{\\text{max}}}{\\theta_{0}}\\leq \\sqrt[n]{ \\alpha }\nFancy!"},"Knowledge/Mathematics/Courses/Statistics/index":{"slug":"Knowledge/Mathematics/Courses/Statistics/index","filePath":"Knowledge/Mathematics/Courses/Statistics/index.md","title":"Statistics","links":[],"tags":[],"content":""},"Knowledge/Mathematics/Courses/Stochastic-Processes-and-Applications/Brownian-Motion":{"slug":"Knowledge/Mathematics/Courses/Stochastic-Processes-and-Applications/Brownian-Motion","filePath":"Knowledge/Mathematics/Courses/Stochastic Processes and Applications/Brownian Motion.md","title":"Brownian Motion","links":["Knowledge/Mathematics/Courses/Stochastic-Processes-and-Applications/Stochastic-Differential-Equations"],"tags":[],"content":"While Stochastic Differential Equations with jumps are continuous-time and continuous-state, they are not quite what you’d imagine as a continuous process\nCan we construct a process with independent and stationary increments that does not have state-dependent coefficients?\nBook Notes\nDefinition\nWe start with assumptions about how this model should work,\n\nX_{0}=0 \nX_{t_{i}}-X_{s_{i}} and X_{t_{j}}-X_{s_{j}} for s_{i}\\leq t_{i}\\leq s_{j}\\leq t_{j} are independent\nThe distribution of X_{t}-X_{s} depends only on t-s\n\\mathbb{E}(X_{t})=0\nX_{t} is a continuous function of t\n\nThese assumptions happen to uniquely describe the process, up to a scaling constant\nWhat’s the distribution of X_{t}? Let’s start with t=1,\nX_{1}=[X_{1 /n}-X_{0}]+[X_{2 /n}-X_{1 /n}]+\\dots+[X_{n/n}-X_{(n-1) /n}] for any n, i.e. X_{1} is the sum of n i.i.d. random variables\nThese increments are also small, meaning M_{n}=\\max\\{ |X_{1 /n}-X_{0}|,\\dots,|X_{n /n}-X_{(n-1) /n}| \\}\\to 0 as n\\to \\infty\nTherefore, X_{t} is normal\nA Brownian motion or a Weiner process with variance parameter \\sigma^{2} is a stochastic process X_{t} taking values in the real numbers satisfying,\n\nX_{0}=0;\nFor any s_{1}\\leq t_{1}\\leq s_{2}\\leq t_{2}\\leq\\dots\\leq s_{n}\\leq t_{n}, the variables X_{t_{1}}-X_{s_{1}},\\dots,X_{t_{n}}-X_{s_{n}} are independent;\nFor any s&lt;t, the random variable X_{t}-X_{s} has a normal distribution with mean 0 and variance (t-s)\\sigma^{2};\nThe paths are continuous, i.e., the function t \\mapsto X_{t} is a continuous function of t\n\nWe include the normal distribution requirement, even though it follows naturally from the definition\nStandard Brownian motion has \\sigma^{2}=1\nWe can also speak of a Brownian motion with X_{0}=x\nWe can look at this motion as a limit of random walks\nSuppose S_{n} is an unbiased random walk on the integers\nS_{n}=Y_{1}+\\dots+Y_{n} where \\mathbb{P}\\{ Y_{i}=1 \\}=\\mathbb{P}\\{ Y_{i}=-1 \\}= \\frac{1}{2}\nWe look at time increments of \\Delta t= 1/N and set W^{(N)}_{k \\Delta t}=N^{-1 /2}S_{k} (N^{-1 /2} normalizes \\text{Var}(S_{N})=N)), so the size of the jump in \\Delta t=1 /N is N^{-1 /2}=(\\Delta t)^{1 /2}\nWe consider the discrete approximation as a process for all values of t (not just intervals of \\Delta t) by linear interpolation, and as N\\to \\infty, the discrete approximation approaches a continuous process which is Brownian motion (the details of this limit are not important here)\nW_{1}^{(N)}=\\frac{S_{N}}{\\sqrt{ N }} approaches a standard normal by CLT and W_{t}^{(N)} approaches a normal distribution with mean 0 and variance t\n\\mathbb{E}(|X_{t+\\Delta t}-X_{t}|^{2})=\\Delta t, meaning the typical size of an increment |X_{t+\\Delta t}-X_{t}| is about \\sqrt{ \\Delta t }\n\\frac{dX_{t}}{dt}=\\lim_{ \\Delta t \\to 0 }\\frac{X_{t+\\Delta t}-X_{t}}{\\Delta t} does not exist, since \\sqrt{ \\Delta t } is much larger than \\Delta t for small values\nThe path of a Brownian motion X_{t} is nowhere differentiable\nThis is strangely non-trivial to prove\nConsider the following two statements,\n\n\\mathbb{P}\\{ X_{t} \\neq 1 \\}=1 for all t\n\\mathbb{P}\\{ X_{t}\\neq 1\\text{ for all }t \\}=1\n\nThe first statement is true, but the second statement is false! X_{0}=0 and likely some X_{t}&gt;1, in which case there is a point that crosses over X_{t}=1\nWe cannot simply take the union of an uncountable set\nAs an illustrative example, consider \\{ -\\infty&lt;Y&lt;\\infty \\}=\\bigcup\\limits_{-\\infty&lt;y&lt;\\infty}\\{ Y=y \\}\nMarkov Property\nLet \\mathcal{F}_{t} represent the information contained in X_{s},s\\leq t\nE(X_{t}\\mid \\mathcal{F}_{s})=E(X_{s}\\mid \\mathcal{F}_{s})+E(X_{t}-X_{s}\\mid \\mathcal{F}_{s})=X_{s}\nRewritten, E(X_{t}\\mid \\mathcal{F}_{s})=X_{s}=E(X_{t}\\mid X_{s})\nThis is the Markov property of Brownian motion, that we can predict X_{t} given all the information up through time s with just the value of the Brownian motion at time s\nThe general form of the Markov property is E[f(X_{t})\\mid \\mathcal{F}_{s}]=E[f(X_{t})\\mid X_{s}] for s\\leq t\nFor Brownian motion, this follows from a stronger property, that X_{s+t}-X_{s} is a Brownian motion independent of \\mathcal{F}_{s}, i.e. X_{s+t} is a Brownian motion starting at X_{s}\np_{t}(x,y)=\\frac{1}{\\sqrt{ 2\\pi t }}e^{-(y-x)^{2} /2t}, since X_{t}-X_{0} is \\mathcal{N}(0,t)\nThis satisfies Chapman-Kolmogorov, written in the continuous form as p_{s+t}(x,y)=\\int_{-\\infty}^{\\infty} p_{s}(x,z)p_{t}(z,y)\\, dz\nWe want a stronger guarantee than the Markov property\nA random variable T\\in [0,\\infty] is a stopping time for Brownian motion if \\{ T\\leq t \\} is measurable with respect to \\mathcal{F}_{t}\nWe mainly consider T_{x}=\\inf\\{ t:X_{t}=x \\}\n\\mathcal{F}_{T} represents the information contained up through a stopping time T\nY_{t}=X_{t+T}-X_{T} is the process beyond time T\nStrong Markov Property: Y_{t} is a Brownian motion independent of \\mathcal{F}_{t}\nThis is more powerful because it states independence from a non-fixed time (a random variable)\nExample: Calculate the probability that there is some 0\\leq t\\leq 1 where X_{t}\\geq 1\nLet T=T_{1} be the first time the Brownian motion equals 1\n\\{ X_{t}\\geq 1\\text{ for some }0\\leq t\\leq 1 \\}=\\{ T\\leq 1 \\}\n\\mathbb{P}\\{ T=1 \\}\\leq \\mathbb{P}\\{ X_{1}=1 \\}=0 so \\mathbb{P}\\{ T\\leq 1 \\}=\\mathbb{P}\\{ T&lt;1 \\}\n\\mathbb{P}\\{ X_{1}\\geq 1 \\}=\\int_{1}^{\\infty} \\frac{1}{\\sqrt{ 2\\pi }}e^{-x^{2}/2}\\, dx\n=\\mathbb{P}\\{ T\\leq 1 \\}\\mathbb{P}\\{ X_{1}\\geq 1\\mid T\\leq 1 \\}\nThis step uses the Strong Markov property,\nX_{1}-X_{T}=X_{1}-1, so \\mathbb{P}\\{ X_{1}-1\\geq 0\\mid T\\leq 1 \\}= 1/2\n\\mathbb{P}\\{ T\\leq 1 \\}=2\\mathbb{P}\\{ X_{1}\\geq 1 \\}=2\\int_{1}^{\\infty} \\frac{1}{\\sqrt{ 2\\pi }}e^{-x^{2}/2}\\, dx\nReflection Principle: Suppose X_{t} is a Brownian motion with \\sigma^{2} starting at a and a&lt;b. Then for any t&gt;0, \\mathbb{P}\\{ X_{s}\\geq b\\text{ for some }0\\leq s\\leq t \\}=2\\mathbb{P}\\{ X_{t}\\geq b\\mid X_{0}=a \\}=2\\int_{b}^{\\infty} \\frac{1}{\\sqrt{ 2\\pi t\\sigma^{2} }}e^{-(x-a)^{2}/(2\\sigma^{2}t)}\\, dx\nExample:\n\\mathbb{P}\\{ X_{s}=0\\text{ for some }1\\leq s\\leq t \\}\nSuppose X_{1}=b&gt;0\nThe probability X_{s}=0 for some 1\\leq s\\leq t is the same as the probability X_{s}\\leq-b for some 0\\leq s\\leq t-1, which by symmetry is the same as the probability X_{s}\\geq b for some 0\\leq s\\leq t-1\n\\mathbb{P}\\{ X_{s}=0\\text{ for some }1\\leq s\\leq t \\mid X_{1}=b\\}=2\\mathbb{P}\\{ X_{t}\\geq b\\mid X_{0}=0 \\}=2\\int_{b}^{\\infty} \\frac{1}{\\sqrt{ 2\\pi(t-1) }}e^{-x^{2}/2(t-1)}\\, dx\nWe average over all values of b,\n\\mathbb{P}\\{ X_{s}=0\\text{ for some }1\\leq s\\leq t \\}=\\int_{-\\infty}^{\\infty} p_{1}(0,b)\\mathbb{P}\\{ X_{s}=0\\text{ for some }1\\leq s\\leq t \\mid X_{1}=b\\}\\, db=2\\int_{0}^{\\infty} \\frac{1}{\\sqrt{ 2\\pi }}e^{-b^{2}/2}\\left[ 2\\int_{b}^{\\infty}{ \\frac{1}{\\sqrt{ 2\\pi(t-1) }}e^{-x^{2}/2(t-1)}}\\, dx \\right]\\, db\nWe can ultimately solve this with some substitutions, leading to 1-\\frac{2}{\\pi }\\text{tan}^{-1} \\frac{1}{\\sqrt{ t-1 }} \nScaling Properties\nConsider Z=\\{ t:X_{t}=0 \\}\nThis is an interesting “fractal” of the real line\nScaling Properties: Suppose X_{t} is a standard Brownian motion,\n\nY_{t}=a^{-1/2}X_{at} is a standard Brownian motion, for a&gt;0\nY_{t}=tX_{1 /t} is a standard Brownian motion\n\nWe showed before \\mathbb{P}\\{ Z\\cap[1,t]\\neq \\emptyset \\}=1-\\frac{2}{\\pi}\\tan ^{-1} \\frac{1}{\\sqrt{ t-1 }} which approaches 1 as t\\to \\infty\nAlong with the strong Markov property, this show that the Brownian motion returns to the origin infinite times\nWhat happens near t=0? Our second scaling property implies that Y_{t}=tX_{1 /t} also returns to the origin infinite times near the origin\nZ has many interesting properties,\n\nIt is closed, i.e. t_{i }\\in Z and t_{i}\\to t implies t\\in Z (from continuity)\n0 is not an isolated point, i.e. there are positive numbers t_{i} such that t_{i}\\to0 (in fact no points are isolated)\nZ looks like the Cantor set\n\nWhat is the dimension of Z?\nBoth Hausdorff dimensions and box dimensions can give rise to fractional dimensions, which are often referred to as fractal dimensions (we talk about box dimension)\nSuppose we have a bounded set A in \\mathbb{R}^{d} and we need to cover A with d-dimensional balls with diameter \\epsilon… How many balls will it take? A line segment will take \\epsilon ^{-1} balls, a square \\epsilon^{-2}, and a standard k-dimensional set will take \\epsilon^{-k} balls\nThus, the box dimension of a set A is the number D such that for small \\epsilon the number of balls of diameter \\epsilon needed to cover A is on the order of \\epsilon^{-D}\nExample:\nConsider the Cantor set A, obtained by recursively removing the middle third of each interval\nWe can cover A with 2^n intervals of length 3^{-n}\n2^{n}=(3^{-n})^{-D}\nD=\\frac{\\ln2}{\\ln 3}\nHow do we apply this to Z? Let’s cover Z_{1}=Z \\cap[0,1] with intervals of diameter \\epsilon = 1/n\nWe will consider the n intervals \\left[ \\frac{k-1}{n}, \\frac{k}{n} \\right], k=1,2,\\dots ,n\nA particular interval is needed if Z_{1}\\cap[(k-1) /n,k /n]\\neq \\emptyset\nP(k,n)=\\mathbb{P}\\left\\{  Z_{1}\\cap\\left[ \\frac{k-1}{n}, \\frac{k}{n} \\right] \\neq \\emptyset \\right\\}\nAssume k\\geq 1, Y_{t}=((k-1) /n)^{-1 /2}X_{nt(k-1)} is a standard Brownian motion\nSo P(k,n)=\\mathbb{P}\\left\\{  Y_{t}=0\\text{ for some }1\\leq t\\leq \\frac{k}{k-1}  \\right\\}\nThis example solved the required probability, so we see P(k,n)=1 -\\frac{2}{\\pi}\\tan ^{-1}\\sqrt{ k-1 }\n\\sum_{k=1}^{n}P(k,n)=\\sum_{k=1}^{n}\\left[ 1-\\frac{2}{\\pi}\\tan ^{-1}\\sqrt{ k-1 } \\right]\n\\tan ^{-1} \\frac{1}{t}=\\frac{\\pi}{2}-t+O(t^{2})\nFor large x, \\tan ^{-1}x\\approx \\frac{\\pi}{2}-\\frac{1}{x}\nHence, \\sum_{k=1}^{n}P(k,n)\\approx 1+\\sum_{k=2}^{n}\\frac{2}{\\pi \\sqrt{ k-1 }}\\approx \\frac{2}{\\pi}\\int_{1}^{n} (x-1)^{-1/2}\\, dx \\approx \\frac{4}{\\pi}\\sqrt{ n }\nSo it takes on the order of \\sqrt{ n } intervals of length \\frac{1}{n} to cover Z_{1}\n\\sqrt{ n }= \\left( \\frac{1}{n} \\right)^{-D}\nD=\\frac{1}{2}\nBrownian Motion in Several Dimensions\nSuppose X_{t}^1,\\dots,X_{t}^{d} are independent one-dimensional standard Brownian motions, then we call X_{t}=(X_{t}^{1},\\dots,X_{t}^{d}) a standard d-dimensional Brownian motion\nThis satisfies the following,\n\nX_{0}=0\nFor any s_{1}\\leq t_{1}\\leq s_{2}\\leq t_{2}\\leq\\dots\\leq s_{n}\\leq t_{n}, the vector-valued random variables X_{t_{1}}-X_{s_{1}},\\dots,X_{t_{n}}-X_{t_{n-1}} are independent\nThe random variable X_{t}-X_{s} has a joint normal distribution with mean 0 and covariance matrix (t-s)\\mathbf{I}, i.e. joint density \\left( \\frac{1}{\\sqrt{ 2\\pi r }}e^{-x^{2}_{1} /2r} \\right)\\dots\\left( \\frac{1}{\\sqrt{ 2\\pi r }}e^{-x_{d}^{2}/2r} \\right)=\\frac{1}{(2\\pi r)^{d/2}}e^{-|x|^{2}/2r}\nX_{t} is a continuous function of t\n\np_{t}(x,y),\\;x,y \\in \\mathbb{R}^{d} is the probability density of X_{t} assuming X_{0}=x\np_{t}(x,y)=\\frac{1}{(2\\pi t)^{d/2}}e^{-|y-x|^{2}/2t}\nThis still satisfies Chapman-Kolmogorov, p_{s+t}(x,y)=\\int_{R^{d}}^{} p_{s}(x,z)p_{t}(z,y)\\, dz_{1}\\dots dz_{d}\nBrownian motion is related to the theory of diffusion\nSuppose that a large number of particles are distributed in \\mathbb{R}^{d} according to density f(y) and let f(t,y) denote the density at time t, such that f(0,y)=f(y)\nIf a particle starts at position x, the probability density for its position at time t is p_{t}(x,y)\nIntegrating gives f(t,y)=\\int_{\\mathbb{R}^{d}}^{} f(x)p_{t}(x,y)\\, dx_{1}\\dots dx_{d}\nSymmetry tells us p_{t}(x,y)=p_{t}(y,x)\nf(t,y)=\\int_{\\mathbb{R}^d}^{}f(x)p_{t}(y,x) \\, dx_{1}\\dots dx_{d}\nAssuming X_{0}=y, this is the expected value of f(X_{t}), which we denote as f(t,y)=\\mathbb{E}^{y}[f(X_{t})]\nWe’d like to derive a differential equation for f(t,x)\nConsider \\frac{\\partial f}{\\partial t} with t=0,\\;d=1\nThe Taylor series for a function f is f(y)=f(x)+f&#039;(x)(y-x)+ \\frac{1}{2}f&#039;&#039;(x)(y-x)^{2}+{o}((y-x)^{2}), where o(\\cdot) is an error term such that \\frac{o((y-x)^{2})}{(y-x)^{2}}\\to0 as y\\to x\n\\frac{\\partial f}{\\partial t}|_{t=0}=\\lim_{ t \\to 0 } \\frac{1}{t}\\mathbb{E}^x[f(X_{t})-f(X_{0})]\n=\\lim_{ t \\to 0 } \\frac{1}{t}\\left[ f&#039;(x)\\mathbb{E}^{x}[X_{t}-x]+ \\frac{1}{2}f&#039;&#039;(x)\\mathbb{E}^{x}(X_{t}-x)^{2}+o((X_{t}-x)^{2}) \\right]\n\\mathbb{E}^x[X_{t}-x]=0 and \\mathbb{E}^x[(X_{t}-x)^{2}]=\\text{Var}(X_{t})=t\nSo we get \\frac{\\partial f}{\\partial t}|_{t=0}= \\frac{1}{2}f&#039;&#039;(x)\nThe same thing works for all t, so \\frac{\\partial f}{\\partial t}= \\frac{1}{2}\\frac{\\partial^{2}f}{\\partial x^{2}}\nGeneralizing d, \\frac{\\partial f}{\\partial t}= \\frac{1}{2}\\Delta f where \\Delta f(t,x_{1},\\dots,x_{d})=\\sum_{i=1}^{d}\\frac{\\partial^{2}f}{\\partial x^{2}_{i}} is the Laplacian\nThis is known as the heat equation\nBrownian motion with variance parameter \\sigma^{2}=D yields \\frac{\\partial f}{\\partial t}=\\frac{D}{2}\\Delta f with the diffusion constant\nConsider a bounded region B with boundary \\partial B with initial heat distribution f(x),\\;x \\in B and fixed temperature g(y),\\;y\\in \\partial B\nIf u(t,x) denotes the temperature at x at time t, then u(t,x) satisfies\n\n\\frac{\\partial u}{\\partial t}= \\frac{D}{2}\\Delta u,\\;x \\in B\nu(t,x)=g(x),\\;x \\in \\partial B\nu(0,x)=f(x),\\;x \\in B\n\nWe can write this in terms of Brownian motion,\nLet X_{t} be a d-dimensional Brownian motion with \\sigma^{2}=D, and let \\tau=\\tau_{\\partial B}=\\tau=\\text{inf}\\{ t:X_{t}\\in \\partial B \\},\nu(t,x)=\\mathbb{E}^x[f(X_{t})I\\{ \\tau&gt;t \\}+g(X_{\\tau})I\\{ \\tau\\leq t \\}]\nAs t\\to \\infty, we reach a steady-state distribution v(x),\n\n\\Delta v(x)=0,\\; x \\in B\nv(x)=g(x),\\;x \\in \\partial B\n\nv(x)=\\lim_{ t \\to \\infty }u(t,x)=\\mathbb{E}^{x}[g(X_{\\tau})]\nExample:\nLet d=1, B=(a,b) with 0\\leq a\\leq b&lt;\\infty such that \\partial B=\\{ a,b \\}, and take a&lt;x&lt;b\nConsider \\tau=\\text{inf}\\{ t:X_{t}=a\\text{ or }b \\} where X_{t} is a standard Brownian motion\nLet g be the function on \\partial B where g(a)=0 and g(b)=1\nv(x)=\\mathbb{E}^{x}[g(X_{\\tau})]=\\mathbb{P}^{x}\\{ X_{\\tau}=b \\}\n\\frac{d^{2}v}{dx^{2}}=0 for a&lt;x&lt;b\nv(a)=0\nv(b)=1\nv(x)=\\frac{x-a}{b-a}\nWe can extrapolate recurrence in one dimension from this,\na=0\n\\lim_{ b \\to \\infty }v(x)=0, i.e. x always returns to 0\nRecurrence and Transience\nSuppose X_{t} is a standard d-dimensional Brownian motion and let B=B(R_{1},R_{2}) be the annulus B=\\{ x \\in \\mathbb{R}^d:R_{1}&lt;|x|&lt;R_{2} \\} with \\partial B=\\{ x \\in \\mathbb{R}^d:|x|=R_{1}\\text{ or }|x|=R_{2} \\}\nLet f(x)=f(x,R_{1},R_{2}) be the probability that a standard Brownian motion starting at x hits the sphere defined by R_{2} before R_{1}\n\\tau=\\tau_{\\partial B}=\\text{inf}\\{ t:X_{t}\\in \\partial B \\}\nf(x)=\\mathbb{E}^x[g(X_{\\tau})] where g(y)=\\mathbb{1}(|y|=R_{2})\nTo start, we have,\n\\Delta f(x)=0, x \\in  B and f(y)=0 for |y|=R_{1} and 1 for |y|=R_{2}\nDue to symmetry, we can also write f(x)=\\phi(|x|) for some \\phi\nWritten in spherical coordinates,\n\\Delta \\phi(r)= \\frac{d^{2}\\phi}{dr^{2}}+ \\frac{d-1}{r} \\frac{d\\phi}{dr}\n\\phi&#039;&#039;(r)+ \\frac{d-1}{r}\\phi&#039;(r)=0\n\\phi(r)= \\begin{cases}c_{1}\\ln r+c_{2}, &amp; d=2 \\\\ c_{1}r^{2-d}+c_{2}, &amp; d\\geq 3\\end{cases}\nWe use our boundary conditions to obtain,\nf(x)=\\frac{\\ln |x|-\\ln R_{1}}{\\ln R_{2}-\\ln R_{1}}, for d=2\nf(x)=\\frac{R_{1}^{2-d}-|x|^{2-d}}{R_{1}^{2-d}-R_{2}^{2-d}}, for d\\geq 3\nGiven this equation, what’s the probability that d= 2 Brownian motion never returns to the disc of radius \\epsilon?\n\\lim_{ R_{2} \\to \\infty }\\mathbb{P}^x\\{ |X_{t}|=R_{2}\\text{ before }|X_{t}|=\\epsilon \\}=\\lim_{ R_{2} \\to \\infty } \\frac{\\ln|x|-\\ln\\epsilon}{\\ln R_{2}-\\ln\\epsilon}=0\nSo for discs with positive radius, Brownian motion is recurrent, but can d=2 Brownian motion return to exactly X_{t}=0?\n\\lim_{ \\epsilon \\to 0 }\\left[ 1- \\frac{\\ln|x|-\\ln\\epsilon}{\\ln R_{2}-\\ln\\epsilon} \\right]=0\nNo, Brownian motion in two dimensions is neighborhood recurrent but not point recurrent\nFor d\\geq 3,\n\\lim_{ R_{2} \\to \\infty } \\frac{\\epsilon^{2-d}-|x|^{2-d}}{\\epsilon^{2-d}-R_{2}^{2-d}}=1-\\left( \\frac{\\epsilon}{|x|} \\right)^{d-2}&gt;0\nSo Brownian motion with d\\geq 3 is transient\nFractal Nature\nWe already talked about the fractal nature of the zero-set of Brownian motion, but there is more to say about fractals\nLet A=\\{ x \\in \\mathbb{R}^d :X_{t}=x\\text{ for some }t\\} with d\\geq 2\nWe can bound the set A_{1}=A\\cap \\{ x:|x|\\leq 1 \\}\nHow many balls of diameter \\epsilon do we need to cover A_{1}?\nFor d=2, consider the ball of radius 1, which needs on order of \\epsilon^{-d} to be covered. By our argument in the previous section, the Brownian motion will visit all open balls hence we need all \\epsilon^{-2} and the box dimension of A is 2\nFor d&gt; 2, the probability a ball with diameter \\epsilon /2 around a point x is visiting is (\\epsilon /2|x|)^{d-2} which is about a constant times \\epsilon^{d-2} (considering the scaling in terms of \\epsilon), thus the total number of balls needed is \\epsilon^{d-2}\\epsilon^{-d}=\\epsilon^{-2}\nWe conclude the path of a d-dimensional Brownian motion (d\\geq 2) has fractal dimension two\nClosely related to the fractal nature of Brownian motion is the scaling rule listed earlier, if X_{t} is a standard one-dimensional Brownian motion and b&gt;0, then Y_{t}=b^{-1/2}X_{bt} is also a standard Brownian motion\nWe examine whether there is some process by which Y_{t}=b^{-\\lambda}X_{bt} has the same distribution as X_{t} for \\lambda \\neq 1/2, i.e. whether there are possible alternative scaling rules\nIf X_{t} has finite variance then \\text{Var}(X_{1})=\\text{Var}[X_{1 /n}+(X_{ 2 /n}-X_{1 /n})+\\dots+(X_{n /n}-X_{(n-1)/n})]\n=n\\text{Var}(X_{1 /n})=n\\text{Var}(n^{-\\lambda}X_{1})=n^{1-2\\lambda}\\text{Var}(X_{1}), implying \\lambda= 1 /2\nLet M_{n}=\\max\\{ |X_{1 /n}|,|X_{2 /n}-X_{ 1 /n}|,\\dots,|X_{n /n}-X_{(n-1) /n}| \\}\nIf the paths have jumps then \\mathbb{P}\\{ M_{n}\\geq\\epsilon \\} ought not to go to zero as n\\to \\infty, however they should still be limited at some point\n\\mathbb{P}\\{ M_{n}\\leq r \\}=\\mathbb{P}\\{ |X_{j /n}-X_{(j - 1) / n}|\\leq r \\text{ for }j=1,\\dots,n\\}\n=\\mathbb{P}\\{ |X_{ 1 / n}|\\leq r \\}^{n}\n=\\mathbb{P}\\{ n^{-\\lambda}|X_{1} |\\leq r\\}^{n}=\\mathbb{P}\\{ |X_{1}|\\leq rn^{\\lambda} \\}^{n}\nThe book implies we can extrapolate from this that \\mathbb{P}\\{ |X_{1}|\\geq y \\}\\sim cy^{-1 /\\lambda} is a good candidate\nIf \\lambda&lt; 1/2 then this X_{1} has a finite variance and therefore \\lambda= 1 /2\nFor \\lambda &gt; 1 / 2, there are examples called symmetric stable distributions, with corresponding processes called symmetric stable processes\nThese densities can only be given explicitly for \\lambda=1, which is the Cauchy distribution with f(x)= \\frac{1}{\\pi(1+x^{2})}\nBrownian Motion with Drift\nConsider a d-dimensional Brownian motion X_{t} with variance parameter \\sigma^{2} starting at x \\in \\mathbb{R}^d, let \\mu \\in  \\mathbb{R}^d and Y_{t}=X_{t}+t\\mu\nWe call Y_{t} a d-dimensional Brownian motion with drift \\mu and variance parameter \\sigma^{2} starting at x\nY_{t} satisfies,\n\nY_{0}=x\nDisjoint increments are independent\nY_{t}-Y_{s} has a normal distribution with mean \\mu(t-s) and covariance matrix \\sigma^{2}(t-s)\\mathbf{I}\nY_{t} is a continuous function of t\n\nThe motion of Y_{t} is essentially a “straight line” with fluctuations, i.e. \\mathbb{E}(Y_{t})=t\\mu\np_{t}(x,y)= \\frac{1}{(2\\pi\\sigma^{2}t)^{d / 2}}e^{-|y-x-t\\mu|^{2} / 2 t \\sigma^{2}} which satisfies the Chapman-Kolmogorov equation\nConsidering d=1, t=0, we write f in a Taylor series about x,\nf(y)=f(x)+f&#039;(x)(y-x)+ \\frac{1}{2}f&#039;&#039;(x)(y-x)^{2}+o((y-x)^{2})\n\\mathbb{E}^{x}[f(Y_{t})]=f(x)+f&#039;(x)\\mathbb{E}^{x}[Y_{t}-x]+ \\frac{1}{2}f&#039;&#039;(x)\\mathbb{E}^{x}[(Y_{t}-x)^{2}]+o(\\mathbb{E}(Y_{t}-x)^{2})\n\\mathbb{E}^{x}[Y_{t}-x]=\\mathbb{E}[X_{t}+t\\mu]=t\\mu\n\\mathbb{E}^{x}[(Y_{t}-x)^{2}]=\\mathbb{E}[(X_{t}+t\\mu)^{2}]=(t\\mu)^{2}+\\sigma^{2}t\n\\frac{\\partial f}{\\partial t}|_{t=0}=\\lim_{ t \\to 0 }\\frac{\\mathbb{E}^{x}[f(Y_{t})]-\\mathbb{E}^x[f(Y_{0})]}{t}=\\mu f&#039;(x)+\\frac{\\sigma^{2}}{2}f&#039;&#039;(x)\nCompared to the earlier calculations without drift, the addition of drift added a first derivative\nIn d dimensions, \\frac{\\partial f}{\\partial t}= \\sum_{i=1}^{d}\\mu_{i}\\frac{\\partial f}{\\partial x_{i}}+\\frac{\\sigma^{2}}{2}\\Delta f\nClass Lecture Notes\nw_{2\\epsilon}=(w_{2\\epsilon}-w_{\\epsilon})+(w_{\\epsilon}-w_{0})\nIf this is a Levy process, we’ve expressed w_{2\\epsilon} as a sum of i.i.d. variables\nw_{t}=w_{t /n}+(w_{2t /n}-w_{t /n})+(w_{3t /n}-w_{2t /n})+\\dots+(w_{nt /n}-w_{(n-1)t /n}) for any n\nSo we call w_{t} a sum of “many” independent random variables, which by CLT implies it’s Gaussian\n\\mathbb{E}[w_{t}]=n\\mathbb{E}[w_{t/n}]\n\\text{Var}(w_{t})=n\\text{Var}(w_{t /n})\nTheorem: f(nt)=nf(t)\\iff f(t)=kt\nThis tells us expectation and variance are linear functions,\n\\mathbb{E}[w_{t}]=\\mu t\n\\text{Var}(w_{t})=\\sigma^{2}t\nIs it possible to construct this fractal-like process? Yes, but it’s complicated\nHere’s some facts,\n\nw_{t} is continuous everywhere but is differentiable nowhere\n|w_{t}-w_{s}| is proportional to \\sqrt{ |t-s| }\nw_{t} is a Brownian motion\ntw_{1 /t} is a Brownian motion\n\\frac{1}{\\sqrt{ a }}w_{at} is also a Brownian motion!\nw_{t} is reversible\nw_{t} is recurrent, meaning it crosses every level infinite many times at a given interval (since it’s scale invariant)\n\nTheorem: w_{t} is a standard Brownian motion \\iff(w_{t})_{t\\geq 0} is Gaussian, \\mathbb{E}[w_{t}]=0, and \\mathbb{E}[w_{t}\\cdot w_{s}]=\\min(t,s)\nThis theorem makes it pretty easy to show these facts\nIn 2-dimensions, BM is neighborhood-recurrent. In 3-dimensions, it is transient…\nStochastic Calculus\nThe goal is to define differential equations subject to random Brownian fluctuations\ndx_{t}=f(x_{t})dt+g(x_{t})dW_{t}\nx_{t}=x_{0}+\\int_{0}^{t} f(x_{s})\\, ds+\\int_{0}^{t} g(x_{s})\\, dW_{s}\nWe can say \\int f(s)\\, dg(s)\\approx\\lim\\sum f(t)(g(t_{i+1})-g(t_{i})), assuming g is smooth enough\nAnd \\sum|g(t_{i+1})-g(t_{i})|^{2} must be &lt;\\infty\nBrownian motion doesn’t satisfy either of these requirements!\nHowever, there is a limit to \\sum_{i=1}^{n}f(t_{i})(w_{t_{i}+1}-w_{t_{i}})"},"Knowledge/Mathematics/Courses/Stochastic-Processes-and-Applications/Introduction":{"slug":"Knowledge/Mathematics/Courses/Stochastic-Processes-and-Applications/Introduction","filePath":"Knowledge/Mathematics/Courses/Stochastic Processes and Applications/Introduction.md","title":"Introduction","links":[],"tags":[],"content":"No textbook, but recommended books include Stochastic Processes by Richard Bass (a bit more theoretical) and Introduction to Stochastic Processes by Gregory F. Lawler (a bit more intuitive)\nNotes,\n\n30% of the grade is weekly homework assignments, 30% is a midterm, 40% is the final, and 5% is extra for participation\nThe lowest homework grade will be dropped\nCollaboration is encouraged, but you must write up your own solutions\nHomework returned late will lose 10 points and after 24 hours will not be accepted, but you can get some leeway as long as you email\n\nA stochastic process defines random variables changing over time\nBrownian motion was first described by Titus Lucretius Carus, ~50 BC. He describes the random dust particles that one sees when sunbeams shine into rooms. It’s ultimately attributed to Robert Brown, who observed pollen in water under a microscope.\nStochastic processes occur in the context of quantum mechanics, neuroscience, stock prices, and many other naturally occurring processes. Many processes follow trends over time, but have lots of noise.\nIs anything really random? Good question! The goal of this class is to find ways to model/approximate processes that are hard to predict, which is not to say whether those processes are random or not.\nDefinition: A stochastic process is a collection of random variables indexed by time\n(X_{t})_{t\\in \\mathscr{C}}\nX_{t} for t fixed is a real value on space E\nIf E is discrete, we have pmf p(X_{t}=a)=p_{t}(a)\nIf E is continuous, we have P(X_{t}\\in [a,b])=\\int_{a}^{b} p_{t}(u)\\, du where p_{t}(u) represents the pdf\nWe can also have discrete or continuous time\nThe random walk is an example of discrete time and discrete space\nContinuous time is generally more difficult to work with\nThe discrete dynamical system is defined as x_{0}\\in E, x_{n+1}=F(x_{n})\nThere are no exact solutions to dynamical solutions except for some special cases of linear functions\nx_{n+1}=\\alpha x_{n}\\implies x_{n}=\\alpha^nx_{0}\nWe will next try to solve x_{n+2}=\\alpha x_{n+1}+\\beta x_{n}"},"Knowledge/Mathematics/Courses/Stochastic-Processes-and-Applications/Stochastic-Differential-Equations":{"slug":"Knowledge/Mathematics/Courses/Stochastic-Processes-and-Applications/Stochastic-Differential-Equations","filePath":"Knowledge/Mathematics/Courses/Stochastic Processes and Applications/Stochastic Differential Equations.md","title":"Stochastic Differential Equations","links":["Knowledge/Computer-Science/Courses/Computational-Neuroscience/Firing-Rate-Models"],"tags":[],"content":"Consider a differential equation with changes at exponentially distributed intervals. For example, regular interest rates in an account (jumps with monthly interest), the level of a body of water (jumps with rain), stock prices (jumps at opening), or neuron Firing-Rate Models (jumps with action potential)\nJump Stochastic Differential Equations (driven by Poisson processes) are continuous-state space and continuous time stochastic processes\n\n(x(t))_{t\\geq 0} with discontinuities (jumps) at random times, where the duration between jumps are independent exponential random variables \\lambda (a Poisson process with times N_{\\lambda}(t))\nBetween jumps, \\frac{dx(t)}{dt}=f(x(t))\nAt a jump, x(t)=x(t^-)+\\beta(x(t^-)), where x(t^-) is the value “right before” the jump\n\nWe use a bit of an abuse of notation,\ndx(t)=f(x(t))\\cdot dt+\\beta(x(t))\\cdot dN_{\\lambda}(t)\nSolutions of SDEs with jumps satisfy the Markov property, \\mathbb{P}[x(t)\\in A\\mid x(s),0\\leq s\\leq \\tau]=\\mathbb{P}[x(t)\\in A\\mid x(\\tau)]\nItô’s Lemma: If \\Phi is a differentiable function and x is the solution of an SDE with jumps, then \\Phi(x(t)) is a solution of the SDE d\\Phi(x(t))=\\Phi&#039;(x(t))f(x(t))\\cdot dt+\\left[ \\Phi\\left( x(t)+\\beta(x(t)) \\right) -\\Phi(x(t))\\right]\\cdot dN_{\\lambda}(t)\nThis seems logical to me\nTheorem: If dx(t)=f(x(t)) dt+\\beta(x(t))dN_{\\lambda}(t), then \\frac{d\\mathbb{E}[x(t)]}{dt}=\\mathbb{E}[f(x(t))]+\\lambda \\mathbb{E}[\\beta(x(t))]\nWe can use this and Itô’s Lemma to solve a lot of problems\nFor example, we can often use Itô with a clever \\Phi (like \\log(x)) and then integrate for a direct solution for x\nExamples:\ndx(t)=\\alpha\\cdot dt+\\beta\\cdot dN_{\\lambda}(t)\nx(t)=x(0)+\\alpha t+\\beta N_{\\lambda}(t)\ndx(t)=-\\alpha x(t)\\cdot dt+\\beta x(t)\\cdot dN_{\\lambda}(t)\nx(t)=x(0)e^{-\\alpha t}(1+\\beta)^{N_{\\lambda}(t)}\nDefinition: A compound Poisson process is a Poisson process with i.i.d. jumps. We can drive an SDE with this process instead of N_{\\lambda}(t). In this case, the amplitude of the jump is \\zeta\\beta(x(t^-)) where \\zeta is a random variable.\nIf Y denotes our compound Poisson process with rate \\lambda and jump size law \\zeta, then we write the associated SDE as dx(t)=f(x(t))\\cdot dt+\\beta(x(t))\\cdot dY_{t}\nWe can write an Itô formula where the jump of \\Phi(x(t)) is \\Phi(x+\\zeta\\beta(x(t)))-\\Phi(x(t))\n\\frac{d\\mathbb{E}[x(t)]}{dt}=\\mathbb{E}[f(x(t))]+\\lambda \\cdot\\mathbb{E}[\\zeta]\\cdot\\mathbb{E}[\\beta(x(t))]\n\\frac{d\\mathbb{E}[\\Phi(x(t))]}{dt}=\\mathbb{E}[\\Phi&#039;(x(t))f(x(t))]+\\lambda \\mathbb{E}[\\Phi(x(t)+\\zeta\\beta(x(t)))-\\Phi(x(t))]"},"Knowledge/Mathematics/Courses/Stochastic-Processes-and-Applications/index":{"slug":"Knowledge/Mathematics/Courses/Stochastic-Processes-and-Applications/index","filePath":"Knowledge/Mathematics/Courses/Stochastic Processes and Applications/index.md","title":"Stochastic Processes","links":["Knowledge/Mathematics/Courses/Stochastic-Processes-and-Applications/Introduction"],"tags":[],"content":"Introduction"},"index":{"slug":"index","filePath":"index.md","title":"Home","links":[],"tags":[],"content":"This site is a collection of technical notes, write ups, and other thoughts. I will be updating this irregularly.\nThis is not a portfolio. My project experience is listed on GitHub, and my professional experience is listed on LinkedIn."}}